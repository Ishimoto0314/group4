{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4章 ニューラルネットワークの学習\n",
    "# 4.2 損失関数\n",
    "# 損失関数はニューラルネットワークの性能の「悪さ」を示す指標\n",
    "#  現在のニューラルネットワークが教師データに対してどれだけ適合していないか、教師データに対してどれだけ一致していないかということを表す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 2乗和誤差\n",
    "\n",
    "# この配列の要素は、最初のインデックスから順に、数字の「０」、「１」、「２」・・に対応\n",
    "\n",
    "# ここでニューラルネットワークの出力であるyは、ソフトマックス関数の出力\n",
    "#  「０」の確率は0.1、「１」の確率は0.05、「２」の確率は0.6といったようなことを表す\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "# tは教師データ\n",
    "# この教師データは正解となるラベルを1、それ以外を0とする\n",
    "#  ここではラベルの「２」が1なので、正解は「２」であることを表す\n",
    "#  なお、正解ラベルを1として、それ以外は0で表す表記法をone-hot表現という\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2乗和誤差はニューラルネットワークの出力と正解となる教師データの各要素の差の2乗を計算し、その総和を求める\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「２」を正解とする\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例１：「２」の確率が最も高い場合(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例２：「７」の確率が最も高い場合(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この実験の結果で示されるように、一つ目の例の損失関数のほうが小さくなっており、教師データとの誤差が小さいことがわかる\n",
    "#  つまり、一つ目の例の方が、出力結果が教師データにより適合していることを2乗和誤差は示している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.2 交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差を実装\n",
    "def cross_entropy_error(y, t):\n",
    "    # np.log(0)のような計算が発生した場合、np.log(0)はマイナスの無限大を表す-infとなり、\n",
    "    #  そうなってしまうと、それ以上計算を進めることが出来なくなる\n",
    "    # その防止策として、微小な値を追加して、マイナス無限大を発生させないようにしている\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.3 ミニバッチ学習\n",
    "# ビックデータともなると、すべてのデータを対象とした損失関数を計算するのは現実的ではない\n",
    "#  そこで、データの中から一部を選び出し、その一部のデータを全体の「近似」として利用するーこれをミニバッチ(小さな塊)というー\n",
    "# そのミニバッチごとに学習する。たとえば、60000枚の訓練データの中から100枚を無作為に選びだして、その100枚を使って学習を行う\n",
    "#  このような学習方法をミニバッチ学習という"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir('C:\\\\Users\\\\200286\\\\workspace\\\\github\\\\gitlocalrep\\\\deep-learning-from-scratch\\\\ch03') # カレントディレクトリをch03に変更\n",
    "sys.path.append(os.pardir) # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この訓練データの中からランダムに10枚だけ抜き出す\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "\n",
    "# np.random.choice()を使えば、指定された数字の中からランダムに好きな数だけ取り出すことができる\n",
    "#  たとえば、np.random.choice(60000, 10)とすると、0から60000未満の数字の中からランダムに10個の数字を選び出す\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30995, 59913,  1785, 36578, 31537,  5361, 53045,  4928,  5398,\n",
       "       58076])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後は、このランダムに選ばれたインデックスを指定して、ミニバッチを取り出すだけ。このミニバッチを使って、損失関数を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 [バッチ対応版]交差エントロピー誤差の実装\n",
    "# 先ほど実装した交差エントロピー誤差は一つのデータを対象とした誤差なので、それを改良する\n",
    "\n",
    "# yはニューラルネットワークの出力、tは教師データとする\n",
    "def cross_entropy_error(y , t):\n",
    "    \n",
    "    # yの次元が1の場合、つまり、データひとつあたりの交差エントロピー誤差を求める場合は、データの形状を整形する\n",
    "    if y.sum == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
