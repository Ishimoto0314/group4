{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4章 ニューラルネットワークの学習\n",
    "# 4.2 損失関数\n",
    "# 損失関数はニューラルネットワークの性能の「悪さ」を示す指標\n",
    "#  現在のニューラルネットワークが教師データに対してどれだけ適合していないか、教師データに対してどれだけ一致していないかということを表す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 2乗和誤差\n",
    "\n",
    "# この配列の要素は、最初のインデックスから順に、数字の「０」、「１」、「２」・・に対応\n",
    "\n",
    "# ここでニューラルネットワークの出力であるyは、ソフトマックス関数の出力\n",
    "#  「０」の確率は0.1、「１」の確率は0.05、「２」の確率は0.6といったようなことを表す\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "# tは教師データ\n",
    "# この教師データは正解となるラベルを1、それ以外を0とする\n",
    "#  ここではラベルの「２」が1なので、正解は「２」であることを表す\n",
    "#  なお、正解ラベルを1として、それ以外は0で表す表記法をone-hot表現という\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2乗和誤差はニューラルネットワークの出力と正解となる教師データの各要素の差の2乗を計算し、その総和を求める\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「２」を正解とする\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例１：「２」の確率が最も高い場合(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例２：「７」の確率が最も高い場合(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この実験の結果で示されるように、一つ目の例の損失関数のほうが小さくなっており、教師データとの誤差が小さいことがわかる\n",
    "#  つまり、一つ目の例の方が、出力結果が教師データにより適合していることを2乗和誤差は示している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.2 交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差を実装\n",
    "def cross_entropy_error(y, t):\n",
    "    # np.log(0)のような計算が発生した場合、np.log(0)はマイナスの無限大を表す-infとなり、\n",
    "    #  そうなってしまうと、それ以上計算を進めることが出来なくなる\n",
    "    # その防止策として、微小な値を追加して、マイナス無限大を発生させないようにしている\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.3 ミニバッチ学習\n",
    "# ビックデータともなると、すべてのデータを対象とした損失関数を計算するのは現実的ではない\n",
    "#  そこで、データの中から一部を選び出し、その一部のデータを全体の「近似」として利用するーこれをミニバッチ(小さな塊)というー\n",
    "# そのミニバッチごとに学習する。たとえば、60000枚の訓練データの中から100枚を無作為に選びだして、その100枚を使って学習を行う\n",
    "#  このような学習方法をミニバッチ学習という"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir('C:\\\\Users\\\\200286\\\\workspace\\\\github\\\\gitlocalrep\\\\deep-learning-from-scratch\\\\ch03') # カレントディレクトリをch03に変更\n",
    "sys.path.append(os.pardir) # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この訓練データの中からランダムに10枚だけ抜き出す\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "\n",
    "# np.random.choice()を使えば、指定された数字の中からランダムに好きな数だけ取り出すことができる\n",
    "#  たとえば、np.random.choice(60000, 10)とすると、0から60000未満の数字の中からランダムに10個の数字を選び出す\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20819, 25681, 40848, 19301,  9119, 49210, 23215, 38415,   365,\n",
       "       48489])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後は、このランダムに選ばれたインデックスを指定して、ミニバッチを取り出すだけ。このミニバッチを使って、損失関数を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 [バッチ対応版]交差エントロピー誤差の実装\n",
    "# 先ほど実装した交差エントロピー誤差は一つのデータを対象とした誤差なので、それを改良する\n",
    "\n",
    "# yはニューラルネットワークの出力、tは教師データとする\n",
    "def cross_entropy_error(y, t):\n",
    "    \n",
    "    # yの次元が1の場合、つまり、データひとつあたりの交差エントロピー誤差を求める場合は、データの形状を整形する\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # バッチの枚数で正規化し、1枚あたりの平均の交差エントロピー誤差を計算する\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# また、教師データがラベルとして与えられたとき(one-hot表現ではなく、「２」や「７」といったラベルとして与えられたとき)\n",
    "#  交差エントロピー誤差は次のように実装することができる\n",
    "# 実装のポイントは、one-hot表現でtが0の要素は、交差エントロピー誤差も0であるから、その計算は無視してもよいということ\n",
    "#  言い換えれば、正解ラベルに対して、ニュートラルネットワークの出力を得ることが出来れば、交差エントロピー誤差を計算することができる\n",
    "\n",
    "def cross_entropy_error2(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    # tがone-hot表現のときは、t * np.log(y)で計算していた箇所を、tがラベル表現の場合は、\n",
    "    #  np.log(y[np.arange(batch_size), t] + 1e-7として同じ表現を実現する\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log(y[np.arange(batch_size), t]を簡単に説明すると、\n",
    "#  np.arange(batch_size)は0からbatch_size-1までのNumPy配列を生成する\n",
    "#  tにはラベルが[2, 7, 0, 9, 4]のように格納されているので、y[np.arange(batch_size), t]は、\n",
    "#  各データの正解ラベルに対応するニュートラルネットワークの出力を抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.5 なぜ損失関数を設定するのか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 数値微分\n",
    "# 4.3.1 微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 悪い実装例\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上の実装では、hにはできるだけ小さな値を用いたかったので(できることなら、hを0に無限に近づけたかったので)、\n",
    "#  hには10e-50(「0.00・・・1」の0が50個続く数)という小さな値を用いている。\n",
    "#  しかし、これでは、逆に丸め誤差が問題になってしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で示すように、1e-50をfloat32型(32ビットの浮動小数点数)で表すと、0.0となり、正しく表現できないことがわかる\n",
    "#  つまり、小さすぎる値を用いることはコンピュータで計算するうえで問題になるということ\n",
    "# そこで、一つ目の改善ポイントで、それはこの微小な値hとして10^-4を用いること。10^-4程度の値を用いれば、良い結果が得られることがわかっている\n",
    "\n",
    "# 二つ目の改善ポイントは関数fの差分について\n",
    "#  上の実装では、x＋hとxの間での関数fの差分を計算しているが、そもそも、この計算には誤差が生じることに注意する必要がある\n",
    "#  (x+h)とxの間の傾きに対応するので、真の微分と今回の実装の値は、厳密には一致しない。\n",
    "#  この差異は、hを無限に0へと近づけることが出来ないために生じる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerial_diff2(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.2 数値微分の例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
